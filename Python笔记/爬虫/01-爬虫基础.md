# 01-爬虫基础

### 什么是爬虫：

​    ---通过编写程序，模拟浏览器上网，然后让其去互联网上抓取数据的过程。
​    爬虫的价值：
​        --实际应用
​        --就业

### 爬虫在使用场景中的分类：

​    --通用爬虫：
​        抓取系统重要组成部分。抓取的是一整张页面数据。
​    --聚焦爬虫：
​        是建立在通用爬虫的基础之上的，抓取的是页面中特定的局部内容
​    --增量式爬虫：
​        是检测网站中数据更新的情况。只会抓取网站中最新更新出来的数据。

### 爬虫中的矛与盾：

反爬虫机制：
    门户网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。
反反爬策略：
    爬虫程序可以通过制定相关的策略或者技术手段，
    破解门户网站中具有的反爬机制，从而获取门户网站中的数据。

robots.txt协议：
    君子协议。规定了网站中那些数据可以被爬虫爬取，那些数据不能被爬取。

http协议：     超文本传输协议
    --概念：就是服务器和客户端进行数据交互的一种形式。
常用的请求头信息：request Headers
    user-Agent: 表示请求载体的身份标识
    Connection: 请求完毕以后，是断开连接还是保持连接
常用的响应头信息
    Content-Type: 服务器响应回客户端的数据类型
https协议：        数据加密了
    --安全的超文本传输协议。
数据加密方式：
    --对称秘钥加密
    --非对称秘钥加密
    --证书秘钥加密

### request模块：

#### requests模块：

​    --urllib模块
​    --requests模块

#### requests模块：

​    Python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率高效
​    作用：
​        模拟浏览器发请求。
​    如何使用：（requests模块的编码流程）
​        1 - 指定url
​        2 - 发起请求
​        3 - 获取响应数据
​        4 - 持久化存储
环境的安装：
​    pip install requests
实战编码：
​    -需求：爬取搜狗页面首页的数据

```python
import requests

#爬取搜狗页面首页的数据
if __name__ == '__main__':
	# 指定url
	url = 'https://www.sogou.com/'
    # 发起请求,get方法会返回一个响应对象
    response = requests.get(url=url)
    # 获取响应数据
    # response.text返回该响应对象的字符串形式的响应数据
    page_text = response.text
    # 持久化存储
    with open('./sogou.html', 'w', encoding='utf8') as fp:
        fp.write(page_text)
    print('爬取数据结束！')
```

### request实现网页采集器

指定搜索关键字

UA检测  ：门户网站的服务器会检测对应请求载体的身份标识，

如果检测到请求的载体为某一款浏览器，说明该请求为一个正常的请求
但是，如果检测到请求的载体身份标识不是基于某一款浏览器的，则
标识本次请求为不正常的请求（爬虫），则服务器就很可能拒绝该请求
    UA： User-Agent（用户设置）请求载体的身份标识

```python
import requests

# UA伪装： 让爬虫对应的请求载体身份标识伪装成某一款浏览器
if __name__ == '__main__':
    # UA伪装： 将对应的User-Agent封装到一个字典中
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'
    }

    # url='https://www.sogou.com/sogou?query=于子正'
    url = 'https://www.sogou.com/sogou?'
    # 处理url携带的参数，封装到字典中
    kw = input('enter a word:')
    param = {
        'query': kw
    }
    # 对指定的url发起请求对应的url是携带参数的，并且请求过程中处理了参数
    # 将参数拼装到url后面 = 得到'https://www.sogou.com/sogou'+'query=kw'
    response = requests.get(url=url, params=param, headers=headers)
    # 获取响应数据
    page_test = response.text
    # 持久化存储
    file_name = kw + '.html'
    with open(file_name, 'w', encoding='utf8') as fp:
        fp.write(page_test)
    print(file_name, '保存成功！')
```

#### 百度翻译

```python
# Author: 毛毛
# FileName: 03-百度翻译.py
# Date: 2020/12/18-22:52
# Software: PyCharm
import json

import requests

# post请求
# 响应数据是一组json数据
if __name__ == '__main__':
    # post请求参数和get一致
    post_url = 'https://fanyi.baidu.com/sug'
    # 进行uA伪装
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'
    }
    word = input('请输入您想爬取的单词？')
    data = {
        'kw': word
    }
    # 请求发送
    response = requests.post(url=post_url, data=data, headers=headers)
    # 获取响应数据:json返回的是obj，一个字典对象（如果确认响应数据是json类型的才能使用）
    dict_obj = response.json()
    # print(dict_obj)
    # 持久化存储
    file_name = word + 'json'
    fp = open(file_name, 'w', encoding='utf8')
    json.dump(dict_obj, fp=fp, ensure_ascii=False)
    fp.close()
    print('over！！！！！！！！！succeed!!!!!!')
```

